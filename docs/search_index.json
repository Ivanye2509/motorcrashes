[["index.html", "NYC Motor Vehicle Crashes Analysis Chapter 1 Introduction", " NYC Motor Vehicle Crashes Analysis Maxwell Zhou, Xin Ye, and Kiranmai Vasireddy 2021-11-19 Chapter 1 Introduction A motor vehicle accident can have a large number of factors that lead to it. A collection of a series of such accidents and understanding the pattern (if any) with respect to the location of the accident, the time at which the accident has taken place and other contributing factors like which vehicle has collided with the motor vehicle that led to the accident can help us understand the correlation between these factors and the accident. This information can be of great insight to both the common people and the police officers, if perceived in the right way. Firstly, it helps the common man by giving a caveat to be more cautious while driving around the accident prone area at a particular point of time. Secondly, it would help police officers in two ways - to keep and check on safety of the people, by monitoring the contributing factors of the accident in the accident prone locations at a particular time and also, to manage their manpower well, more police officers can be put to work at accident prone areas compared to the other areas. This is driving factor behind the project that we aim to work on, to study the pattern of motor vehicle accidents that take place in New York City to help understand and organise resources to reduce accidents With this study, we try to answer the following questions: Is the number of people injured or killed by motor vehicle accidents impacted by the time and day at which the accident has occurred. Are there any underlying contributing reasons that are resulting in more accidents? Does the severity of the accident have a correlation with the location at which it has taken place? For instance, do accidents in Manhattan have a higher death ratio compared to accidents in the Bronx? Is there any pattern of the vehicles that hit the motor vehicles? For instance, do they get hit by sedans more than trucks? Analysis of the dataset to answer these questions would give us a better insight about how the common man can be more careful to avoid accidents and also to the police in ensuring our safety. "],["data-sources.html", "Chapter 2 Data sources", " Chapter 2 Data sources Still need to change the format for this chapter later. The dataset chosen for this study is taken from the NYC Open Data. It is owned by NYC Open Data and is provided by the New York Police Department(NYPD). The dataset is a table where each row of the table represents a crash event. It contains the information from all the police reported motor vehicle collisions in NYC. In cases where someone is injured or killed, or a damage worth $1000 a police report is required - MV104-AN. The dataset is subjected to change where there is any update in the police report (MV104-AN) with the revised crash details. It was created in April, 2014 and is a large dataset with 1.84Million entries and 29 columns. Why and how is it collected? The police department implemented TrafficStat using the CompStats model to work towards the safety of people and to maintain uniformity in which data related to motor vehicles is collected. Initially, they used a TAMS- Traffic Management Systems to collect traffic data using a few details around the collision, but later on when they decided to work towards the Vision Zero- to ensure zero fatalities, the MV-104AN form was filled in a much more detailed manner for all the vehicle collisions that occured. The TAMS model was eliminated and FORMS - Finest Online Records Management System was introduced in 2016 to store data electronically. The policemen use a department cellphone or computer to enter all the MV-104AN data fields which are stored in the Department crime data warehouse. This ensures that the entire MV-104AN form with all the details is stored securely, which can be used to do a detailed traffic safety analysis to work towards the Vision Zero goal. The data we would work on: Since the dataset is enormous, we have decided to work on a complete year of data, starting from 1, January 2020 to 31, December 2020. This data contains ______ entries. Studying the recent year of data gives a clear understanding of the latest trends in the collisions, given the growth of traffic every year. Problems/Issues: A few of the fields in the Dataset are missing which are being imputed/eliminated based on the type of question we plan to answer. A brief overview of the dataset: Dataset contains 29 attributes, a brief overview of all these are described below: Column Name Description Crash Date Date of collision Crash Time Time of collision Borough Place of collision Zip Code Postal code where the collision occurred Latitude Latitude on Coordinate System Longitude Longitude on Global Coordinate System On Street Name Street on which collision occurred Cross Street Name Nearest cross Street where collision occurred. Off Street name Street Address Number of people,pedestrians, cyclist, motorist injured Details on the number of people injured in each category Number of people,pedestrians, cyclist, motorist killed Details on the number of people killed in each category Contributing factors vehicle 1,2,3,4,5 Factors which contribute to the collision Collision ID Unique Record code generated by system Vehicle Type Code 1,2,3,4,5 Type of vehicle "],["data-transformation.html", "Chapter 3 Data transformation", " Chapter 3 Data transformation Havent completed the description for Data Transformation Chapter yet. ## Number of Rows: 1829491 ## Number of Columns: 29 ## # A tibble: 5 x 29 ## `CRASH DATE` `CRASH TIME` BOROUGH `ZIP CODE` LATITUDE LONGITUDE LOCATION ## &lt;chr&gt; &lt;time&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 04/14/2021 05:32 &lt;NA&gt; NA NA NA &lt;NA&gt; ## 2 04/13/2021 21:35 BROOKLYN 11217 40.7 -74.0 (40.68358, -~ ## 3 04/15/2021 16:15 &lt;NA&gt; NA NA NA &lt;NA&gt; ## 4 04/13/2021 16:00 BROOKLYN 11222 NA NA &lt;NA&gt; ## 5 04/12/2021 08:25 &lt;NA&gt; NA 0 0 (0.0, 0.0) ## # ... with 22 more variables: ON STREET NAME &lt;chr&gt;, CROSS STREET NAME &lt;chr&gt;, ## # OFF STREET NAME &lt;chr&gt;, NUMBER OF PERSONS INJURED &lt;dbl&gt;, ## # NUMBER OF PERSONS KILLED &lt;dbl&gt;, NUMBER OF PEDESTRIANS INJURED &lt;dbl&gt;, ## # NUMBER OF PEDESTRIANS KILLED &lt;dbl&gt;, NUMBER OF CYCLIST INJURED &lt;dbl&gt;, ## # NUMBER OF CYCLIST KILLED &lt;dbl&gt;, NUMBER OF MOTORIST INJURED &lt;dbl&gt;, ## # NUMBER OF MOTORIST KILLED &lt;dbl&gt;, CONTRIBUTING FACTOR VEHICLE 1 &lt;chr&gt;, ## # CONTRIBUTING FACTOR VEHICLE 2 &lt;chr&gt;, ... ## Number of Rows: 112890 ## Number of Columns: 29 ## CRASH DATE CRASH TIME ## &quot;CRAD&quot; &quot;CRAT&quot; ## BOROUGH ZIP CODE ## &quot;BORO&quot; &quot;ZIPC&quot; ## LATITUDE LONGITUDE ## &quot;LATI&quot; &quot;LONG&quot; ## LOCATION ON STREET NAME ## &quot;LOCA&quot; &quot;ONSN&quot; ## CROSS STREET NAME OFF STREET NAME ## &quot;CRSN&quot; &quot;OFSN&quot; ## NUMBER OF PERSONS INJURED NUMBER OF PERSONS KILLED ## &quot;NUMBEROFPERI&quot; &quot;NUMBEROFPERK&quot; ## NUMBER OF PEDESTRIANS INJURED NUMBER OF PEDESTRIANS KILLED ## &quot;NUMBEROFPEDI&quot; &quot;NUMBEROFPEDK&quot; ## NUMBER OF CYCLIST INJURED NUMBER OF CYCLIST KILLED ## &quot;NOCI&quot; &quot;NOCK&quot; ## NUMBER OF MOTORIST INJURED NUMBER OF MOTORIST KILLED ## &quot;NOMI&quot; &quot;NOMK&quot; ## CONTRIBUTING FACTOR VEHICLE 1 CONTRIBUTING FACTOR VEHICLE 2 ## &quot;CFV1&quot; &quot;CFV2&quot; ## CONTRIBUTING FACTOR VEHICLE 3 CONTRIBUTING FACTOR VEHICLE 4 ## &quot;CFV3&quot; &quot;CFV4&quot; ## CONTRIBUTING FACTOR VEHICLE 5 COLLISION_ID ## &quot;CFV5&quot; &quot;COLL&quot; ## VEHICLE TYPE CODE 1 VEHICLE TYPE CODE 2 ## &quot;VTC1&quot; &quot;VTC2&quot; ## VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 ## &quot;VTC3&quot; &quot;VTC4&quot; ## VEHICLE TYPE CODE 5 ## &quot;VTC5&quot; ## [1] &quot;Date&quot; &quot;Time&quot; &quot;BOR&quot; &quot;ZIP&quot; &quot;LAT&quot; &quot;LONG&quot; &quot;LOC&quot; &quot;ONSN&quot; &quot;CRSN&quot; ## [10] &quot;OFFSN&quot; &quot;#PI&quot; &quot;#PK&quot; &quot;#PEDI&quot; &quot;#PEDK&quot; &quot;#CI&quot; &quot;#CK&quot; &quot;#MI&quot; &quot;#MK&quot; ## [19] &quot;CFV1&quot; &quot;CFV2&quot; &quot;CFV3&quot; &quot;CFV4&quot; &quot;CFV5&quot; &quot;id&quot; &quot;VTC1&quot; &quot;VTC2&quot; &quot;VTC3&quot; ## [28] &quot;VTC4&quot; &quot;VTC5&quot; ## # A tibble: 112,890 x 32 ## Date Time BOR ZIP LAT LONG LOC ONSN CRSN OFFSN `#PI` `#PK` ## &lt;date&gt; &lt;time&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-01-21 15:49 &lt;NA&gt; NA NA NA &lt;NA&gt; BRUC~ &quot;\\u0~ &lt;NA&gt; 0 0 ## 2 2020-12-31 16:30 &lt;NA&gt; NA NA NA &lt;NA&gt; BELT~ &lt;NA&gt; &lt;NA&gt; 0 0 ## 3 2020-12-25 20:19 &lt;NA&gt; NA NA NA &lt;NA&gt; BRON~ &lt;NA&gt; &lt;NA&gt; 0 0 ## 4 2020-04-15 15:20 &lt;NA&gt; NA 40.7 -74.0 (40.~ GOWA~ &lt;NA&gt; &lt;NA&gt; 0 0 ## 5 2020-10-25 02:00 &lt;NA&gt; NA NA NA &lt;NA&gt; BELT~ &lt;NA&gt; &lt;NA&gt; 0 0 ## 6 2020-11-11 16:33 &lt;NA&gt; NA NA NA &lt;NA&gt; TRIB~ &lt;NA&gt; &lt;NA&gt; 0 0 ## 7 2020-04-17 01:50 MANH~ 10019 40.8 -74.0 (40.~ 11 A~ &quot;WES~ &lt;NA&gt; 1 0 ## 8 2020-12-18 07:00 &lt;NA&gt; NA NA NA &lt;NA&gt; HORA~ &quot;EAS~ &lt;NA&gt; 0 0 ## 9 2020-09-06 18:05 &lt;NA&gt; NA 40.8 -73.8 (40.~ WHIT~ &lt;NA&gt; &lt;NA&gt; 0 1 ## 10 2020-12-13 18:35 &lt;NA&gt; NA NA NA &lt;NA&gt; 12 A~ &lt;NA&gt; &lt;NA&gt; 0 0 ## # ... with 112,880 more rows, and 20 more variables: #PEDI &lt;dbl&gt;, #PEDK &lt;dbl&gt;, ## # #CI &lt;dbl&gt;, #CK &lt;dbl&gt;, #MI &lt;dbl&gt;, #MK &lt;dbl&gt;, CFV1 &lt;chr&gt;, CFV2 &lt;chr&gt;, ## # CFV3 &lt;chr&gt;, CFV4 &lt;chr&gt;, CFV5 &lt;chr&gt;, id &lt;dbl&gt;, VTC1 &lt;chr&gt;, VTC2 &lt;chr&gt;, ## # VTC3 &lt;chr&gt;, VTC4 &lt;chr&gt;, VTC5 &lt;chr&gt;, Day &lt;chr&gt;, Month &lt;chr&gt;, Year &lt;chr&gt; "],["missing-values.html", "Chapter 4 Missing values", " Chapter 4 Missing values This is our missing values plot on the entire cleaned 2020 motor vehicle crashes data set. We can see that our dataset contains many different columns (29 columns) and a plethora of missing values across the majority of the columns. Because we have so many columns and rows (112,890 rows), this leads us to have an extremely large amount of missing patterns that appear in our dataset. As a result, it is very hard to see individual rows and columns in our missing data plot because of how great the number of different missing patterns are. The plot below shows the same missing patterns plot, but only including the most frequently appearing missing patterns and also removing the columns that do not have any missing values. This way, we can further examine the missing patterns that appear more commonly in our dataset. Now that we have only selected the columns containing missing values (we already know that all of the columns removed have no missing values at all) and the missing patterns in our dataset that appear more frequently, we can observe many different patterns in the missing values of our dataset. For example, we have currently selected the top 15 missing patterns because these are the missing patterns with over 1000 missing rows. We observe that the 4 features with the highest percentages of missing rows have all of the top 15 missing patterns. ## number_of_features_missing count percentage_missing ## 1 7 28309 25.07662326 ## 2 10 25781 22.83727522 ## 3 8 20253 17.94047303 ## 4 9 15704 13.91088670 ## 5 12 6920 6.12986093 ## 6 11 4347 3.85065108 ## 7 13 3094 2.74072106 ## 8 5 2472 2.18974223 ## 9 6 2441 2.16228187 ## 10 15 849 0.75205953 ## 11 14 795 0.70422535 ## 12 4 748 0.66259190 ## 13 3 632 0.55983701 ## 14 1 296 0.26220214 ## 15 2 232 0.20550979 ## 16 16 12 0.01062982 ## 17 17 5 0.00442909 ## feature_name num_of_missing percentage_missing ## 1 VTC5 112065 99.2692001 ## 2 CFV5 112037 99.2443972 ## 3 VTC4 110106 97.5338825 ## 4 CFV4 109980 97.4222695 ## 5 VTC3 102794 91.0567809 ## 6 CFV3 102183 90.5155461 ## 7 OFFSN 83436 73.9091151 ## 8 CRSN 59616 52.8089290 ## 9 ZIP 39229 34.7497564 ## 10 BOR 39221 34.7426699 ## 11 VTC2 34120 30.2241120 ## 12 ONSN 29454 26.0908849 ## 13 CFV2 24618 21.8070688 ## 14 LAT 8921 7.9023829 ## 15 LONG 8921 7.9023829 ## 16 LOC 8921 7.9023829 ## 17 VTC1 1073 0.9504828 ## 18 CFV1 513 0.4544247 ## 19 Date 0 0.0000000 ## 20 Time 0 0.0000000 ## 21 X.PI 0 0.0000000 ## 22 X.PK 0 0.0000000 ## 23 X.PEDI 0 0.0000000 ## 24 X.PEDK 0 0.0000000 ## 25 X.CI 0 0.0000000 ## 26 X.CK 0 0.0000000 ## 27 X.MI 0 0.0000000 ## 28 X.MK 0 0.0000000 We can see from these two tables that various rows and columns have less and more missing values. For example, we can see that the vehicle type codes and contributing factor vehicles 3-5 have the most rows missing for those columns because many accidents recorded are between 2 cars or 1 car. For the rows, we can see that the majority of rows have between 7 - 10 columns of missing values per row. Fewer rows have very little or very many missing values. In this missing values plot, we can observe the relationship between the missingness of certain rows and the type of the column that the data is missing from. For example, we can see that most of the integer columns have no missing values, except for the zip code column, in which much of those are missing. We can also see missing patterns among the rows as well. For example, we can see that most of the integer columns have no missing values, except for the zip code column, in which much of those are missing. We can also see missing patterns among the rows as well. Besides, all rows that have either latitude, longitude, or location missing also have the other two variables missing, which means those rows are able to be not geographically visualized on a map. However, it seems that the majority of the crashes have the geographic features that suggesting we can later visualize these crashes on the map. "],["results.html", "Chapter 5 Results", " Chapter 5 Results "],["interactive-component.html", "Chapter 6 Interactive component", " Chapter 6 Interactive component "],["conclusion.html", "Chapter 7 Conclusion", " Chapter 7 Conclusion "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
